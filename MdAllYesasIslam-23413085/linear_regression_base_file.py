# -*- coding: utf-8 -*-
"""Linear_Regression_Base_File.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GJxls5jyYRWmAuoj93JVdXcRcQiexDDW
"""

from pandas import  read_csv, DataFrame
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn import linear_model
from  plotly import figure_factory

# # reading and metadata
data1 = read_csv("/content/gcp_final_approved_dataset.csv")
data1.head(7)
data1.shape
print(data1.info())
# #print(data1.describe())

# catagorical columns
print(data1['Resource ID'].value_counts())
print("\n")
print(data1['Service Name'].value_counts())
print("\n")
print(data1['Usage Unit'].value_counts())
print("\n")
print(data1['Region/Zone'].value_counts())
print("\n")
print(data1['Usage Start Date'].value_counts())
print("\n")
print(data1['Usage End Date'].value_counts())

# dropping insignificant variables
data1 = data1.drop(['Resource ID','Service Name','Usage Unit','Region/Zone','Usage Start Date','Usage End Date','Unrounded Cost (INR)','Rounded Cost (INR)'], axis = 1)
data1.info()

#Heatmap
correlation  = data1.corr()
correlation.to_csv('/content/gcp_final_approved_dataset.csv')
f = figure_factory.create_annotated_heatmap(correlation.values,list(correlation.columns),list(correlation.columns),correlation.round(2).values,showscale=True)
f.show()

# dropping based on correlated variables
data1 = data1.drop(['Network Outbound Data (Bytes)'], axis = 1)
data1.info()

# Dividing dataset into label and feature sets
X = data1.drop(['Total Cost (INR)'], axis = 1) # Features
Y = data1['Total Cost (INR)'] # Labels
print(X.shape)
print(Y.shape)

# data Scaling
X_ = StandardScaler().fit_transform(X)
DataFrame(X_)

# Linear Regression (LR)

LinearRegression1 = linear_model.SGDRegressor(random_state = 50, penalty = None) # building
Hparameter1 = {'eta0': [.0001, .001, .01, .1, 1], 'max_iter':[5000,7500,10000,12500]}
grid_search1 = GridSearchCV(estimator=LinearRegression1, param_grid=Hparameter1, scoring='r2', cv=5)
grid_search1.fit(X_,Y)

# results = DataFrame.from_dict(grid_search1.cv_results_)
# print("Cross-validation results:\n", results)
best_parameters = grid_search1.best_params_
print("Best parameters: ", best_parameters)
best_result = grid_search1.best_score_
print("Best result: ", best_result)
best_model = grid_search1.best_estimator_
print("Intercept β0: ", best_model.intercept_)
print(DataFrame(zip(X.columns, best_model.coef_), columns=['Features','Coefficients']))
#print(DataFrame(zip(X.columns, best_model.coef_), columns=['Features','Coefficients']).sort_values(by=['Coefficients'],ascending=False))

# #  Regularization
LinearRegression2 = linear_model.SGDRegressor(random_state = 50, penalty = 'elasticnet') # model building
Hparameter2 = {'eta0': [.0001, .001, .01, .1, 1], 'max_iter':[5000,7500,10000,12500],'alpha': [.001, .01, .1, 1,10, 100], 'l1_ratio': [0,0.25,0.5,0.75,1]}

grid_search2 = GridSearchCV(estimator=LinearRegression2, param_grid=Hparameter2, scoring='r2', cv=5)
grid_search2.fit(X_, Y)

# results = DataFrame.from_dict(grid_search2.cv_results_)
# print("Cross-validation results:\n", results)
best_parameters = grid_search2.best_params_
print("Best parameters: ", best_parameters)
best_result = grid_search2.best_score_
print("Best result: ", best_result)
best_model = grid_search2.best_estimator_
print("Intercept β0: ", best_model.intercept_)
print(DataFrame(zip(X.columns, best_model.coef_), columns=['Features','Coefficients']).sort_values(by=['Coefficients'],ascending=False))

import joblib

joblib.dump(best_model, "model.pkl")

DataFrame(X_[50:65])

My_model = joblib.load("model.pkl")
x=[[1.028101,-0.085861,0.675406,-1.517652,0.967103]]
#predict=My_model.predict(x)[0]
predict=round(My_model.predict(x)[0],2)
print(predict)

price=My_model.predict(X_[0:5])
price
DataFrame(price)